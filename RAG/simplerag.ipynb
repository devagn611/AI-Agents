{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'greet.txt'}, page_content='hellloooo \\n\\nhow are you')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"greet.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/data-science/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314'}, page_content='NLP 101: Word2Vec ‚Äî Skip-gram and CBOWA crash course in word embedding.Ria Kulshrestha¬∑FollowPublished inTDS Archive¬∑7 min read¬∑Nov 24, 2019--9ListenShareWhat does word embedding mean?Word embedding is just a fancy way of saying numerical representation of words. A good analogy would be how we use the RGB representation for colors.Why do we need word embedding?As a human, intuitively speaking, it doesn‚Äôt make much sense in wanting to represent words or any other object in the universe using numbers because numbers are used for quantification and why would one need to quantify words?When in science, we say speed of my car is 45 km/hr we gain a sense of how fast/slow we are driving. If we say my friend is driving at 60 km/hr, we can compare which one of us is going faster. Furthermore, we can calculate where we will be at a certain point in time, when we will reach our destination given we know the distance of our journey etc etc. Similarly, outside of science, we use numbers to quantify a quality, when we quote the price of an object we try to quantify its worth, the size of a garment we try to quantify the body proportions it will fit best.All of these representations make sense because by using numbers we have made analysis and comparisons based on those qualities much much easier. What‚Äôs worth more a shoe or a purse? Well, as different as those two objects are, one way to answer that is to compare their prices. Other than the quantification aspect, there isn‚Äôt any thing else to be gained by this representation.Now that we know numerical representation of objects aids in analysis by quantifying a certain quality, the question is what quality of words do we want to quantify?The answer to that is, we want to quantify the semantics. We want to represent words in such a manner that it captures its meaning in a way humans do. Not the exact meaning of the word but a contextual one. For example, when I say the word see, we know exactly what action ‚Äî the context ‚Äî I‚Äôm talking about, even though we might not be able to quote its meaning, the kind we would find in a dictionary, of the top of our head.What are good quality word embedding and how to generate them?The simplest word embedding you can have is using one-hot vectors. If you have 10,000 words in your vocabulary, then you can represent each word as a 1x10,000 vector.For a simple example, if we have 4 words ‚Äî mango, strawberry, city, Delhi ‚Äî in our vocabulary then we can represent them as following:Mango [1, 0, 0, 0]Strawberry [0, 1, 0, 0]City [0, 0, 1, 0]Delhi [0, 0, 0, 1]There are a few problems with the above approach, firstly, our size of vectors depends on the size of our vocabulary(which can be huge). This is a wastage of space and increases algorithm complexity exponentially resulting in the curse of dimensionality.Secondly, these embedding will be closely coupled to their applications, making transfer-learning to a model using a different vocabulary of the same size, adding/removing words from vocabulary would almost impossible as it would require to re-train the whole model again.Lastly, the entire purpose of creating embedding is to capture the contextual meaning of the words, which this representation fails to do. There is no co-relation between words that have similar meaning or usage.## Current situation Similarity(Mango, Strawberry) == Similarity(Mango, City) == 0## Ideal situationSimilarity(Mango, Strawberry) >> Similarity(Mango, City)** Note: Similarity(a,b) = a.b/(||a||*||b||) Cosine similarityContinuous Bag of Words Model (CBOW) and Skip-gramBoth are architectures to learn the underlying word representations for each word by using neural networks.Source: Exploiting Similarities among Languages for Machine Translation paper.In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. While in the Skip-gram model, the distributed representation of the input word is used to predict the context.A prerequisite for any neural network or any supervised training technique is to have labeled training data. How do you a train a neural network to predict word embedding when you don‚Äôt have any labeled data i.e words and their corresponding word embedding?Skip-gram ModelWe‚Äôll do so by creating a ‚Äúfake‚Äù task for the neural network to train. We won‚Äôt be interested in the inputs and outputs of this network, rather the goal is actually just to learn the weights of the hidden layer that are actually the ‚Äúword vectors‚Äù that we‚Äôre trying to learn.The fake task for Skip-gram model would be, given a word, we‚Äôll try to predict its neighboring words. We‚Äôll define a neighboring word by the window size ‚Äî a hyper-parameter.The word highlighted in yellow is the source word and the words highlighted in green are its neighboring words.Given the sentence: ‚ÄúI will have orange juice and eggs for breakfast.‚Äùand a window size of 2, if the target word is juice, its neighboring words will be ( have, orange, and, eggs). Our input and target word pair would be (juice, have), (juice, orange), (juice, and), (juice, eggs).Also note that within the sample window, proximity of the words to the source word plays no role. So have, orange, and, and eggs will be treated the same while training.Architecture for skip-gram model. Source: McCormickml tutorialThe dimensions of the input vector will be 1xV ‚Äî where V is the number of words in the vocabulary ‚Äî i.e one-hot representation of the word. The single hidden layer will have dimension VxE, where E is the size of the word embedding and is a hyper-parameter. The output from the hidden layer would be of the dimension 1xE, which we will feed into an softmax layer. The dimensions of the output layer will be 1xV, where each value in the vector will be the probability score of the target word at that position. According to our earlier example if we have a vector [0.2, 0.1, 0.3, 0.4], the probability of the word being mango is 0.2, strawberry is 0.1, city is 0.3 and Delhi is 0.4.The back propagation for training samples corresponding to a source word is done in one back pass. So for juice, we will complete the forward pass for all 4 target words ( have, orange, and, eggs). We will then calculate the errors vectors[1xV dimension] corresponding to each target word. We will now have 4 1xV error vectors and will perform an element-wise sum to get a 1xV vector. The weights of the hidden layer will be updated based on this cumulative 1xV error vector.CBOWThe fake task in CBOW is somewhat similar to Skip-gram, in the sense that we still take a pair of words and teach the model that they co-occur but instead of adding the errors we add the input words for the same target word.The dimension of our hidden layer and output layer will remain the same. Only the dimension of our input layer and the calculation of hidden layer activations will change, if we have 4 context words for a single target word, we will have 4 1xV input vectors. Each will be multiplied with the VxE hidden layer returning 1xE vectors. All 4 1xE vectors will be averaged element-wise to obtain the final activation which then will be fed into the softmax layer.Skip-gram: works well with a small amount of the training data, represents well even rare words or phrases.CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.In part II of this post: NLP 101: Negative Sampling and GloVe, we discuss:Negative Sampling ‚Äî a technique to improve the learning without compromising the quality of embeddingAnother word embedding called GloVe that is a hybrid of count based and window based model.ReferencesLecture notes CS224D: Deep Learning for NLP Part-ILecture notes CS224D: Deep Learning for NLP Part-IIMcCormick, C. (2016, April 19). Word2Vec Tutorial ‚Äî The Skip-Gram Model.Other Articles by Me That I think You would Enjoy :DYes, you should listen to Andrej Karpathy, and understand Back propagationEvaluation of an NLP model ‚Äî latest benchmarksUnderstanding Attention In Deep LearningTransformers ‚Äî the basic block for models such as Google‚Äôs BERT and OpenAI‚Äôs GPT.I‚Äòm glad you made it till the end of this article. üéâI hope your reading experience was as enriching as the one I had writing this. üíñDo check out my other articles here.If you want to reach out to me, my medium of choice would be Twitter.Machine LearningNLPArtificial IntelligenceWord2vecAI----9Published in TDS Archive814K Followers¬∑Last published\\xa0Feb 3, 2025An archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.FollowWritten by Ria Kulshrestha1K Followers¬∑7 FollowingAI enthusiast currently exploring SE @Google. Claps/Shares/Comments are appreciatedüíñhttps://twitter.com/Ree_____ReeFollowResponses (9)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "\n",
    "loader=WebBaseLoader(web_path=\"https://medium.com/data-science/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\",\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                        class_=(\"du bh iz ja jb jc\")\n",
    "                     )),)\n",
    "\n",
    "web_doc=loader.load()\n",
    "web_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs\\nand Transfer Learning\\nYuyang Miaoa,‚àó, Harry J. Daviesa, Danilo P. Mandica\\na Department of Electrical and Electronic Engineering, Imperial College London, Exhibition Rd, South Kensington, London, SW7 2BX, United Kingdom\\nAbstract\\nPhotoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable\\ndevices. The PPG signals provide insight into the body‚Äôs circulatory system and can be employed to extract various bio-features,\\nsuch as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations,\\nincluding heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we\\nintroduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis\\nframework which is amplitude-independent and invariant to a ffine transformations. It also requires minimal preprocessing, fuses\\ninformation through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves\\nstate-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation of continuous blood pressure\\nwaveforms.\\nKeywords: Photoplethysmography (PPG), Visibility Graph, Transfer Learning, Graph Theory\\n1. Introduction\\nWith the rapid development of technology, the integration\\nof technology into healthcare, denoted as E-health, is reshap-\\ning the ways in which the healthcare services are provided and\\naccessed (Eysenbach et al., 2001). E-health promises enhanced\\nefficiency in healthcare delivery, potentially reducing the cost\\nwhile simultaneously improving the quality of care.\\nPhotoplethysmography (PPG) sensors play a crucial role in\\ne-health and are widely employed in wearable devices due to\\ntheir non-invasive and portable nature. Through estimation of\\nblood volume at the site of the probe, PPG can be used to esti-\\nmate heart rate (Biagetti et al., 2019), respiration (Motin et al.,\\n2017), blood pressure (Long and Wang, 2023), blood oxygen\\nlevels (Davies et al., 2020) and vascular stiffness, among others.\\nA wide range of applications based on PPG signals include, but\\nare not limited to, cognitive load classification (Davies et al.,\\n2022b; Parreira et al., 2023), blood glucose monitoring (Ham-\\nmour and Mandic, 2023; Zhang et al., 2023) and Chronic Ob-\\nstructive Pulmonary Disease (COPD) diagnosis (Davies et al.,\\n2022a).\\nNumerous studies have focused on extracting respiratory in-\\nformation from PPG signals (Sultan and Saadeh, 2023; Motin\\net al., 2019; Davies and Mandic, 2023). Harry et al. (Davies\\nand Mandic, 2023) applied corr-encoder directly on to the PPG\\nsignal and extracted the breathing envelope and estimated the\\nbreathing rate on it. Muhammad et al. (Sultan and Saadeh,\\n‚àóCorresponding author: Yuyang Miao\\nEmail addresses:ym520@ic.ac.uk (Yuyang Miao),\\nharry.davies14@imperial.ac.uk (Harry J. Davies),\\nd.mandic@imperial.ac.uk (Danilo P. Mandic)\\n2023) proposed using multiple features fore breathing rate ex-\\ntraction. Physio, spectral and statistical features were extracted\\nand selected based on frequency analysis, correlation coe ffi-\\ncients, mutual information and minimal redundancy maximal\\nrelevance (mRMR). Finally the breathing rate was determined\\nusing a Neural Network based on the selected features. Motinet\\nal., among those, (Motin et al., 2019) utilised Empirical Mode\\nDecomposition (EMD) and its variations to decompose PPG\\nsignals into their components. The authors then selected the\\nIntrinsic Mode Functions (IMFs) devoid of artefacts and heart\\nrate information, and applied the Principal Component Anal-\\nysis (PCA) algorithm to the remaining components to extract\\nrespiratory rates.\\nPPG signals can also be utilised to estimate vascular age-\\ning. Dall‚ÄôOlio et al. (Dall‚ÄôOlio et al., 2020) proposed an algo-\\nrithm for vascular ageing classification using PPG signals. The\\nauthors initially removed the trend in the PPG signal using a\\ncentered moving average. The processed PPG signal was then\\ndemodulated using a Hilbert transform, and the envelope was\\nextracted. The final version of the PPG signal was obtained\\nby dividing the demodulated signal by the envelope. A recur-\\nsive peak detection method was subsequently applied to extract\\nwindows of 15 peaks, which were fed to a ResNet module for\\nbinary vascular ageing prediction. Additionally, a Support Vec-\\ntor Machine (SVM) model was constructed using features from\\nmetadata and features extracted from the PPG signals. Hangsik\\net al.(Shin et al., 2022) applied a five-layer convolutional neu-\\nral network directly on the PPG pulses.\\nIn 2009, Suzuki and Oguri conducted pioneering work on\\nmachine learning for blood pressure prediction from PPG (Suzuki\\nand Oguri, 2009). The study utilised physiological features, in-\\ncluding Pulse Width (PW), Transit Time (TW), Dicrotic Wave\\nPreprint submitted to Elsevier January 17, 2024\\narXiv:2305.14062v4  [eess.SP]  16 Jan 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='(DW), and Dicrotic Notch (Dn). An AdaBoost model was trained\\nto predict systolic and diastolic blood pressures. Huang et al.\\n(Huang et al., 2022) transferred the MLP-Mixer from the com-\\nputer vision domain to the biomedical signal domain to predict\\nblood pressure waveform using ECG and PPG signals with full\\nMLP architecture.\\nAlthough the existing works have demonstrated good per-\\nformance, several limitations prevent their more widespread use\\nin practice. These include:\\n‚Ä¢ A considerable amount of preprocessing based on manu-\\nally designed rules is required, such as a manual selection\\nof intrinsic mode functions.\\n‚Ä¢ Existing algorithms are sensitive to parameter choice and\\nmay not generalise well across different datasets.\\n‚Ä¢ The majority of approaches employ the amplitude of the\\nPPG signal as an input feature, which can be easily in-\\nfluenced by factors such as age, skin thickness, and skin\\ntones.\\n‚Ä¢ Numerous methods directly work with features extracted\\nfrom temporal domain PPG signals, making them highly\\nsensitive to signal quality and e ffects of a ffine transfor-\\nmations.\\n‚Ä¢ Many existing solutions necessitate complex feature ex-\\ntraction operations.\\n‚Ä¢ Some algorithms seek to apply computer vision algorithms\\nin the field of biomedical signal processing, which may\\nresult in unavoidable compromises, as these algorithms\\nare primarily designed for processing 2D images.\\nTo address these issues, we propose an algorithm that com-\\nbines the amplitude-independent structure features and transfer\\nlearning techniques, named the VGTL-net. By virtue of the vis-\\nibility graph approach, the time series of the PPG signal is trans-\\nferred into a complex graph network which preserves the struc-\\ntural information while discarding the amplitude information.\\nThis makes it possible to employ the corresponding graph adja-\\ncency matrices (serving as images) in conjunction with transfer\\nlearning. And in this way, the VGTL-net can benefit from:\\n‚Ä¢ Use of only geometric information in the PPG signal,\\nmaking the approach amplitude-independent;\\n‚Ä¢ Robustness to affine transformations by virtue of the vis-\\nibility graph;\\n‚Ä¢ Minimal preprocessing requirements;\\n‚Ä¢ Good generalisation across different tasks and datasets;\\n‚Ä¢ Interpretation on how characteristics of the PPG signals\\nare reflected in the visibility graph;\\n‚Ä¢ Minimal feature extraction and prerpocessing requirements;\\n‚Ä¢ Interpretation on how characteristics of the PPG signals\\nare reflected in the visibility graph;\\n‚Ä¢ Information fusion through the RGB channels.\\nThis article is structured as follows. In Section 2, we first\\ngive examples on how characteristics of the PPG signals are\\nreflected in the visibility graph. Then, we introduce the pro-\\nposed VGTL-net algorithm and outline the visibility graph and\\ntransfer learning techniques. In Section 3, we conduct exper-\\niments on two publicly available datasets using the proposed\\nalgorithm, demonstrating its generalisability in both tasks and\\ndatasets. Finally, in Section 4, we present conclusions dis-\\ncussing the performance and future directions.\\n2. METHODOLOGY\\n2.1. Signal To Graph: Visibility Graph\\nA graph G = (V,E) is defined by a set of vertices V con-\\nnected by a set of edgesE. Figure 1 illustrates a graph where the\\nvertices are represented as dots forming a circle, and the lines\\nconnecting them are the edges of the graph. For simplicity, all\\ngraphs in this work are undirected, which means that all edges\\nare bi-directional. The visibility graph transforms a time series\\ninto a graph (Lacasa et al., 2008), whereby each signal sample\\nyi is considered to be a vertex. There is an edge connecting\\ntwo vertices if there exists a line connecting these two signal\\nsamples which does not intersect with another signal sample;\\nthis can be interpreted as one signal sample seeing another one\\nwithout a third signal sample blocking the path. More formally,\\ntwo vertices (signal samples), ya and yb, are connected by an\\nedge if\\nyc <yb + (ya ‚àí yb) tb ‚àí tc\\ntb ‚àí ta\\n(1)\\nfor any other signal sample yc, where ta, tb and tc are the time\\nindexes corresponding to the signal samplesya, yb and yc. Since\\nwe consider undirected graphs only, all visibility is mutual,\\nwhich means yb can see ya if ya can see yb. Figure 2 gives\\nan example of the transformation from a sampled PPG cycle to\\na visibility graph. Figure 2a is the PPG signal and the red lines\\nconnect signal sample 9 with other signal samples that can be\\nseen by it. These connections form edges that are red in the\\nvisibility graph, which is Figure 2b.\\n2.2. Graph To Image: Adjacency Matrix\\nThe adjacency matrix is used to describe the connectivity of\\na graph. For a graph with N vertices, an adjacency matrix A has\\na size of N √ó N. For a weighted graph, if two vertices Vi and Vj\\nare connected, then Ai,j = wi,j, while Ai,j = 0 if not connected.\\nThe weight wi,j may be calculated using some specific rules, for\\nexample, distance, correlation, etc. For an unweighted graph,\\nwi,j will always be unity or zero. Note that the adjacency matrix\\ncan be considered as a grayscale image for a weighted graph\\nand a black-and-white image for an unweighted graph, which\\nis a form suitable as an input to a standard 2D convolutional\\nneural network. Figure 3 gives an example of a time series sig-\\nnal (top left), an unweighted graph generated from the signal\\n(top right), and its corresponding adjacency matrix, viewed as\\nboth a matrix (bottom right) and an image (bottom left). Our\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='Example Graph\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\nFigure 1: An example graph, with nodes which are indexed and form a circle.\\nThe lines connecting the nodes are the edges of the graph.\\nproposed framework operates specifically with the adjacency\\nmatrix of the visibility graph. To this end, the time series PPG\\nsignals are first transformed into visibility graphs and then fur-\\nther transformed into images in the form of adjacency matrices.\\n2.3. Visibility Graph: Invariance To Affine Transformation\\nThe visibility graphs are invariant to affine transformations\\n(Lacasa et al., 2008); in other words, the visibility graphs are\\ninvariant to horizontal and vertical scaling, horizontal and ver-\\ntical translation, as well as the addition of any linear trend to\\nthe original signal. Figure 4 provides an example, whereby the\\noriginal signal (Figure 4a) is first moderately transformed (Fig-\\nure 4b):\\ny(t) = 0.3x(t) (2)\\nthen heavily transformed (Figure 4c):\\ny(t) = x(t) + 10000t + 13 (3)\\nwhere x(t) is the original PPG signal andy(t) is the transformed\\nsignal.\\nObserve that even the heavily affine transformed signal has\\nprecisely the same visibility graph and the same adjacency ma-\\ntrix. For better illustration, we first found the signal sample that\\nhas the highest visibility in the original signal and denoted the\\nindex as i. Next, in all three signals, we connected the ith signal\\nsample with the signal samples that can be seen by them. Then,\\nwe highlighted the points in the adjacency matrices generated\\nby the connections in red. It is evident that with di fferent lev-\\nels of affine transformations, the visibility relationship does not\\nchange, and thus the visibility graph does not change either.\\n9\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nA PPG Signal\\n0 5 10 15 20 25 30 35 40\\n(a)\\nSamples\\nGenerated Visibility Graph\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n910\\n1112\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28 29\\n30 31 32\\n33\\n34\\n35\\n36\\n37\\n38\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n910\\n1112\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28 29\\n30 31 32\\n33\\n34\\n35\\n36\\n37\\n38\\n(b)\\nFigure 2: An example of a visibility graph. (a): A PPG pulse and the red lines\\ndesignate for the visibility of the signal sample 9. (b): The generated visibility\\ngraph and the red lines are the edges generated by the visibility of the signal\\nsample 9.\\n2.4. The PPG signal as a Visibility Graph\\nWe now demonstrate the e ffects of various breathing pat-\\nterns, heart rates, and age on the adjacency matrices of the vis-\\nibility graphs of the corresponding PPG signals. The examples\\nshow that visibility graphs can capture the structural character-\\nistics of PPG signals, providing a foundation for further analy-\\nsis. These results suggest that visibility graphs could potentially\\nbe used to analyse PPG signals and extract important physiolog-\\nical information.\\n2.4.1. Breathing Modulation\\nThe PPG signal, a quantification of changes in blood vol-\\nume through the absorbance of light, is subject to modulation\\nby alterations in respiratory patterns. These modulations stem\\nfrom variations in venous return, stroke volume, tissue volume,\\nand respiratory sinus arrhythmia (Pimentel et al., 2015; Davies\\net al., 2022a). The impact of respiratory activity modulation on\\nPPG signals can be categorised into three distinct groups: am-\\nplitude modulation (AM), baseline wandering (BW), and fre-\\nquency modulation (FM). Notably, BW modulation, which arises\\nfrom fluctuations in venous pressure, is the most readily dis-\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='A PPG Signal\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n0 5 10 15 20 25 30 35 40\\nSamples\\nGenerated Visibility Graph\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n910\\n1112\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28 29\\n30 31 32\\n33\\n34\\n35\\n36\\n37\\n38\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n910\\n1112\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28 29\\n30 31 32\\n33\\n34\\n35\\n36\\n37\\n38\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n0\\nFigure 3: Pathway from a time series to an image. (top left): The original\\nsignal and the visibility between signal samples. (top right): The visibility\\ngraph generated from the input signal. (bottom right): The adjacency matrix\\nwhich corresponds to the visibility graph. (bottom left): The black-and-white\\nimage generated from the adjacency matrix.\\ncernible characteristic within the raw PPG signal (Davies et al.,\\n2022a).\\nTo investigate the effects of deep breathing on the PPG sig-\\nnal, we conducted an experiment whereby a subject initially\\nbreathed normally and then commenced deep breathing. Fig-\\nure 5 depicts the resulting PPG signal segments and their cor-\\nresponding adjacency matrices. Observe the BW modulation,\\na sinusoidal waveform shape, in the PPG signal. This mod-\\nulation introduced additional peaks that were not present in\\nthe normal breathing PPG signal, thus giving certain pulses of\\nthe PPG signal additional visibility, which was manifested as\\nwing shapes in the adjacency matrix. The signal samples with\\nthe highest visibility in both normal and heavy breathing were\\nidentified and were connected to the signal samples that can\\nbe seen by them in red lines. These connections were denoted\\nin the adjacency matrices as red. It can be concluded that the\\nheavy breathing introduced high visibility to some signal sam-\\nples which can be seen as large wing shapes in the adjacency\\nmatrices.\\n2.4.2. Heart rates\\nAn additional easily observable feature of PPG is the heart\\nrate (HR). For illustration, the PPG signal of the subjects was\\ninitially recorded at rest and subsequently after exercising. Fig-\\nure 6 shows the differences in the corresponding adjacency ma-\\ntrices of the PPG segments with different heart rates. The wing-\\nshaped elements along the diagonal signify peaks in the PPG\\nsignal. The individual pulses are represented alternately as red\\nand blue. The sequence of colours in the plot is red, blue, red,\\nblue, and so on. The wing shapes in the adjacency matrices\\nshare the same colour as the corresponding pulses for visuali-\\nsations. Observe that the number of wing shapes increases for\\nfast heart rate PPG signal with the same length as the normal\\nrate PPG signal.\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\n2000\\nPPG With Fast Heartrate\\n0 50 100 150 200 250\\nSamples\\nAdjacency Matrix With Fast Heartrate\\n(a)\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\n2000\\nPPG With Normal Heartrate\\n0 50 100 150 200 250\\nSamples\\nAdjacency Matrix With Normal Heartrate\\n (b)\\nFigure 6: Two segments of PPG signal with di fferent heart rates and their cor-\\nresponding adjacency matrices generated from the visibility graphs. (a): fast\\nheart rate; (b) low heart rate. The wing shapes in the adjacency matrices share\\nthe same colour as the corresponding pulses for visualisations. It is clear that\\nthe number of wing shapes increases for a fast heart rate PPG signal with the\\nsame length as the normal rate PPG signal.\\n2.4.3. Vascular Ageing\\nThe shape of the PPG signal also varies with age, with\\nthe dicrotic notch less pronounced as age increases, yielding\\na smoother PPG signal (Yousef et al., 2012). Figure 7 o ffers\\nan example of how the visibility graph can determine subjects‚Äô\\nvascular ageing. Figure 7a depicts the PPG signal and adja-\\ncency matrix of a subject aged 13, while Figure 7b displays a\\n49 year old subject‚Äôs PPG signal and adjacency matrix; note\\nthat the adjacency matrices exhibit significant di fferences. For\\na younger subject, the PPG signal is less smooth and has more\\nnotches, yielding the adjacency matrix with more white pixels\\nwhich indicates that frequently two nodes can see each other.\\nConversely, due to the smoothness of the PPG signal of the\\nolder subject, most of the data points cannot see each other,\\nproducing an image less filled with white. To illustrate the in-\\ncreased visibility of a younger subject, the signal samples with\\nthe highest visibility in both young and old PPGs are selected\\nand connected with the signal samples they could see using red\\nlines. The red lines are also reflected in the adjacency matri-\\nces in red points. It is obvious that the PPG of a young subject\\nhas larger wings in the adjacency matrix due to the increased\\nvisibility compared with the PPG of the old subject.\\nIn the previous section, we illustrated that to a certain extent\\nphysiological activities could be reflected in the adjacency ma-\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='0\\n50\\n100\\n150\\n200\\n250\\nOriginal PPG\\n5 10 15 20 25 30 35\\nSamples\\nAdjacency Matrix of Original PPG\\n(a)\\n0\\n50\\n100\\n150\\n200\\n250\\nModerate Affine Transformed PPG\\n5 10 15 20 25 30 35\\nSamples\\nAdjacency Matrix of Moderate Affine Transformed PPG\\n (b)\\n (c)\\nFigure 4: (a): Adjacency matrices of the visibility graphs of the original PPG pulse. (b): The PPG pulse under moderate a ffine transformation. (c): The PPG pulse\\nunder heavy affine transformation. Although the PPG pulse becomes unidentifiable after a ffine transformations, the visibility graphs and the adjacency matrices\\nremain the same, showing the visibility graph to be affine invariant.\\ntrices of the corresponding visibility graph. We next introduce\\nthe concept of transfer learning and show how it can be applied\\nto visibility graphs.\\n2.5. Transfer Learning\\nDeep learning has achieved impressive results in the field\\nof computer vision. However, state-of-the-art models often re-\\nquire large amounts of data for training, leading to the issue\\nof data dependence. Transfer learning o ffers a solution to this\\nproblem by training models on large datasets to obtain a set\\nof pre-trained weights, which are then fine-tuned on smaller\\ndatasets. This significantly reduces the required amount of new\\ndata and allows large deep learning models to be applied to a\\nbroader range of fields and scenarios (Tan et al., 2018). Our\\nstudy employed three pretrained models based on emperical\\ntests, the sizes of the datasets and the nature of the tasks: the\\nVGG19 model (Simonyan and Zisserman, 2014) , the Coatnet\\n(Dai et al., 2021), and the Convnext v2 (Woo et al., 2023) . All\\nthree pretrained models were trained on the ImageNet dataset\\n(Deng et al., 2009).\\n2.6. Proposed Learning Scheme: VGTL-net\\nThe proposed learning scheme, termed the VGTL-net (Visi-\\nbility Graph and Transfer Learning), is introduced with the aim\\nof being capable of classification and regression tasks of arbi-\\ntrary size. It comprises three steps: i) the PPG signal (or any\\nother signal) is segmented into fixed-length windows or a fixed\\nnumber of pulses; ii) the segmented PPG signals are then trans-\\nformed into the corresponding adjacency matrices and further\\nconsidered as grayscale or black-and-white images; iii) the ad-\\njacency matrices images are fed to the three channels of the\\npretrained model as an RGB image to give the final output.\\n3. Experiments and Results\\nWe conducted two experiments on PPG signals, to validate\\nthe proposed VGTL-net approach. The first experiment con-\\nsidered the prediction of systolic and diastolic blood pressure,\\nwhile the second experiment aimed to predict vascular ageing.\\n3.1. Blood Pressure Estimation\\n3.1.1. Dataset\\nThe dataset came from the University of California Irvine\\n(UCI) Machine Learning Repository which is a preprocessed\\nversion of MIMIC II dataset (Kachuee Mohamad and Mahdi,\\n2015). The dataset contains PPG, ECG and arterial BP (ABP)\\nsignals all sampled at 125Hz. The dataset was further processed\\nto select acceptable records:\\n‚Ä¢ Sample entropies were calculated for all trails and trials\\ndid not fulfil selected thresholds were removed.\\n‚Ä¢ The trials with accepted entropy values were segmented\\ninto windows of 224 sample points which is suitable for\\nthe pretrained models.\\n‚Ä¢ A plateau detection was performed, windows with plateaus\\nlonger than five points were deleted.\\n‚Ä¢ The systolic and diastolic blood pressures were extracted\\nfrom the windows left. Windows with extreme blood\\npressure values were deleted. Since the MIMIC dataset\\nwas recorded on patients in ICU under critical condi-\\ntions, the accepted values of blood pressure were relaxed\\n(80 <S BP<200, 40 <DBP <120).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='Adjacency Matrix With Normal Breathing\\n400\\n600\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\nPPG With Normal Breathing\\n0 100 200 300 400 500 600 700 800\\nSamples\\n(a)\\nAdjacency Matrix With Heavy Breathing\\n200\\n400\\n600\\n800\\n1000\\n1200\\n1400\\n1600\\nPPG With Heavy Breathing\\n0 100 200 300 400 500 600 700 800\\nSamples\\n(b)\\nFigure 5: Example PPG waveforms and the corresponding adjacency matrices. (a): Normal breathing PPG signal and the adjacency matrix; (b): Heavy breathing\\nPPG signal and the adjacency matrix. Red lines connect the signal sample with the highest visibility and the samples can be seen by it. The red pixels in the\\nadjacency matrices represent edges introduced by the red lines. The baseline wandering introduced by heavy breathing gives some signal samples more visibility,\\nthus referring to more red lines and broader wing-shape elements in the adjacency matrix.\\nTable 1 shows the statistics of the processed dataset.\\nMin Max STD Mean\\n(mmHg) (mmHg) (mmHg) (mmHg)\\nDBP 50 119.9 10.6 66\\nMAP 60.4 168.4 13.5 90.2\\nSBP 80 199.9 22 134.2\\nHR 46.3 234.3 42.6 117.2\\nTable 1: BP and Heart Rate Ranges in the Database\\nFigure 8 shows three examples of bad waveforms that were\\nremoved. The top and bottom row are windows of PPG sig-\\nnals with abnormal entropy values. The middle row illustrates\\na typical example of blood pressure waveform with plateaus.\\n3.1.2. Aim of the Experiment\\nThe goal of this experiment was to predict the systolic and\\ndiastolic blood pressures from the PPG and ECG signals. The\\nPPG and ECG signals were transformed into adjacency matri-\\nces and the model predicted the systolic and diastolic blood\\npressures based on images transformed from these adjacency\\nmatrices.\\n3.1.3. RGB Fusion\\nAs mentioned above, the PPG signal and ECG signal were\\nboth segmented into windows of 224 sample points. We can\\nutilize the fact that the input image has three channels, namely\\nthe red, green and blue channels. Di fferent information can be\\ntransformed into adjacency matrices for different channels.\\nThe images were created with di fferent information in the\\nRGB channels. The pulse transit time calculated from ECG\\nand PPG (Kachuee et al., 2015) and the derivative of the PPG\\nsignal (El-Hajj and Kyriacou, 2021) were often used as input\\nfeatures to estimate the blood pressures. To also encode this\\ninformation in the generated images, the inputs to the red, green\\nand blue channels are the PPG signal, the ECG signal and the\\nfirst-order derivative of the PPG signal. The pulse transit time\\ncan be extracted by analysing the shift between the red and the\\ngreen elements in the image, which are induced by the PPG and\\nECG signals respectively. In figure 9, it is manifest that the trial\\ncorresponding the right image clearly has a longer pulse transit\\ntime compared with the left image trial. The blue component\\ncorresponds to the first-order derivative of the PPG signal.\\nAs for the pretrained models, after empirical tests, two mod-\\nels outperform the others, the Convnext V2 (Woo et al., 2023)\\nand the Coatnet (Dai et al., 2021). We trained two VGTL-net\\nmodels based on these two pretrained models. The classifica-\\ntion layers of these two pretrained models were removed and\\nthe left parts were used as feature extractors. Then for each pre-\\ntrained model, two MLPs of size (F√ó1024√ó512√ó1) were added\\nas top layers to get the systolic and diastolic blood pressures,\\nwhere F represent the feature size of that pretrained model.\\nDuring training, the weights of the pretrained models and the\\ntop layers were updated. Furthermore, after the training of these\\ntwo VGTL-nets, we trained a third VGTL-net model based on\\nthese two pretrained models. The trained Convnext V2 and the\\nCoatnet feature extractors were extracted the the generated fea-\\ntures were concatenated. Then two MLPs were added to form\\na VGTL-net. Finally, only the MLPs were trained to get further\\nrefined results.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='0\\n50\\n100\\n150\\n200\\n250\\n300\\nPPG of young subject\\n0 5 10 15 20 25 30\\nSamples\\nAdjacency Matrix of young subject\\n(a)\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nPPG of old subject\\n0 5 10 15 20 25 30\\nSamples\\nAdjacency Matrix of old subject\\n (b)\\nFigure 7: Age-related characteristics of PPG signals and the corresponding ad-\\njacency matrices. (a): 13 years old subject (b): 49 years old subject. It is\\nobvious that the young PPG have larger wings due to the increased visibility\\ncompared with the old PPG\\nFigure 9: Two examples of input images for blood pressure prediction. It can\\nbe seen from the deviation between the red and green wings that the image on\\nthe right corresponds to PPG and ECG signals with higher pulse transit time\\ncompared with the signals the left image represents.\\n3.1.4. Results and Discussion\\nTable 2 shows the results of the systolic and diastolic blood\\npressure predictions in terms of mean absolute error (MAE),\\nmean error (ME) and Pearson correlation coe fficient. It con-\\ntains other recent relevant research which also works on pro-\\ncessed MIMIC II datasets and the three approaches we pro-\\nposed, namely VGTL-net with Convnext v2, Coatnet and the\\nconcatenation of both of them as mentioned before.\\n3.2. Vascular Ageing\\n3.2.1. Dataset\\nThe dataset used for vascular ageing prediction was the ‚ÄùReal-\\nWorld PPG Dataset‚Äù from Mendeley (Siam, 2019). This dataset\\nconsists of PPG signals for 35 healthy subjects, with 50-60 PPG\\n0 50 100 150 200 250\\nSamples\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPPG With Abnormal Entropy\\n0 50 100 150 200 250\\nSamples\\n80\\n100\\n120\\n140\\n160\\n180\\nBlood Pressure With Plateaus\\n0 50 100 150 200 250\\nSamples\\n50\\n100\\n150\\nBlood Pressure With Abnormal Entropy\\nFigure 8: Top Row: One example of PPG signal with abnormal entropy; Middle\\nRow: One example of blood pressure signal with plateaus; Bottom Row: One\\nexample of blood pressure signal with abnormal entropy.\\nsignals per subject. The duration of each PPG signal is 6 sec-\\nonds, with a sampling rate of 50 Hz. The ages of subjects range\\nfrom 10 to 75 years old. The dataset is divided into the training,\\nvalidation, and test datasets.\\n3.2.2. Aim of The Experiment\\nThis experiment utilised the proposed VGTL-net scheme to\\npredict the age of subjects based on their PPG signals. The pre-\\ndiction was conducted in the regression and classification set-\\ntings. For the regression task, the goal of the VGTL-net was to\\npredict the exact age of the subject, while for the classification\\ntask, the goal of the VGTL-net was to determine the age group\\nof the subject, namely 0-20, 20-30, 30-40, and 40 +.\\n3.2.3. Data Preparation and Model Architecture\\nThe PPG signals were segmented into individual peaks, with\\nevery training, validation or test sample containing only one\\npeak. As the heart rate can change both between and within\\nsubjects, an average length of 50 samples was selected (1 sec-\\nond). Segments longer than 50 samples were truncated, and\\nthose shorter than 50 were padded with zeros. Figure 10 illus-\\ntrates such padding and truncation.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='Model SBP DBP\\nMAE (mmHg) ME (mmHg) œÅ MAE (mmHg) ME (mmHg) œÅ\\nRong et al.(Rong and Li, 2021) (PPG) 5.59 -1.13 ¬± 7.25 ‚úï 3.36 -0.14 ¬± 4.48 ‚úï\\nEl-Hajj and Kyriacou (PPG) (El-Hajj and Kyriacou, 2021) 4.51 ¬± 7.81 -0.48 ¬± 9.16 0.89 2.61 ¬± 4.41 -0.49 ¬± 5.10 0.86\\nSlapnicar et al.(PPG) (SlapniÀácar et al., 2019) 9.43 ‚úï ‚úï 6.88 ‚úï ‚úï\\nHuang et al.(PPG + ECG) (Huang et al., 2022) 3.52 ¬± 5.10 -0.379 0.961 2.13 ¬± 3.07 -0.587 0.939\\nMiao et al.(PPG + ECG) (Miao et al., 2020) ‚úï -0.11 ¬± 9.99 0.88 ‚úï -0.03 ¬± 6.36 0.71\\nWang et al.(PPG + ECG) (Wang et al., 2020) 3.95 ¬± 4.38 ‚úï ‚úï 2.14 ¬± 2.40 ‚úï ‚úï\\nBaker et al.(PPG + ECG) (Baker et al., 2021) 4.41 ¬± 6.11 ‚úï 0.8 2.91 ¬± 4.23 ‚úï 0.85\\nEsmaelpoor et al.(PPG + ECG) (Esmaelpoor et al., 2020) 3.97 ¬± 5.55 1.91 0.954 2.10 ¬± 2.84 0.67 0.950\\nLong et al.(PPG + ECG) (Long and Wang, 2023) 3.98 ¬± 4.62 -0.17 0.9564 2.33 ¬± 2.95 -0.24 0.9340\\nProposed Model (CoAtNet) (PPG + ECG) 3.46 ¬± 4.22 -0.19 ¬± 5.46 0.967 2.11 ¬± 2.51 -0.02 ¬± 3.29 0.950\\nProposed Model (ConvNeXt V2) (PPG + ECG) 3.29 ¬± 4.20 0.03 ¬± 5.34 0.969 1.99 ¬± 2.49 -0.03 ¬± 3.19 0.953\\nProposed Model (Concatenate) (PPG + ECG) 3.11 ¬± 3.92 -0.07 ¬± 5.01 0.973 1.94 ¬± 2.37 -0.5 ¬± 3.01 0.959\\nTable 2: The numeric results of the VGTL-net and other works.\\nModel SBP DBP\\n‚â§ 5 mmHg ‚â§ 10 mmHg ‚â§ 15 mmHg ‚â§ 5 mmHg ‚â§ 10 mmHg ‚â§ 15 mmHg\\nRong et al.(Rong and Li, 2021) (PPG) 54.1% 86.6% 94.9% 82.9% 94.9% 98.3%\\nEl-Hajj and Kyriacou (PPG) (El-Hajj and Kyriacou, 2021) ‚úï ‚úï ‚úï ‚úï ‚úï ‚úï\\nSlapnicar et al.(PPG) (SlapniÀácar et al., 2019) ‚úï ‚úï ‚úï ‚úï ‚úï ‚úï\\nHuang et al.(PPG + ECG) (Huang et al., 2022) 71.23% 92.28% 98.92% 88.07% 98.65% 99.78%\\nMiao et al.(PPG + ECG) (Miao et al., 2020) 50.07% 76.40% 90.39% 65.66% 89.77% 96.63%\\nWang et al.(PPG + ECG) (Wang et al., 2020) ‚úï ‚úï ‚úï ‚úï ‚úï ‚úï\\nBaker et al.(PPG + ECG) (Baker et al., 2021) 67.66% 89.82% 96.82% 82.79% 96.12% 99.58%\\nEsmaelpoor et al.(PPG + ECG) (Esmaelpoor et al., 2020) 73.7% 93.7% 97.7% 92.9% 99.2% 99.9%\\nLong et al.(PPG + ECG) (Long and Wang, 2023) 74.47% 95.52% 97.05% 89.57% 97.70% 99.13%\\nProposed Model (CoAtNet) (PPG + ECG) 79.64% 94.71% 97.91% 91.82% 98.39% 99.49%\\nProposed Model (ConvNeXt V2) (PPG + ECG) 81.63% 95.23% 98.00% 93.05% 98.53% 99.47%\\nProposed Model (Concatenate) (PPG + ECG) 83.07% 95.84% 98.28% 93.52% 98.74% 99.54%\\nGrade Absolute Di fference between the Standard and Test results (%)\\nA 80 90 95\\nB 65 85 95\\nC 45 75 90\\nD Worth than C\\nTable 3: The British Hypertension Society standard (BHS) and the accuracy of the proposed VGTL-net and other works on blood pressure prediction.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='0 5 10 15 20 25 30 35 40 45 50\\nSamples\\n-300\\n-200\\n-100\\n0\\n100\\n200\\n300\\n400\\n500\\nPulses with different lengths plotted together\\nFigure 10: PPG pulses of different lengths visualised in the same graph. Pulses\\nlonger than 50 data points (orange) are truncated and pulses shorter than 50 data\\npoints (blue, purple and amber) are zero-padded.\\nNext, the PPG pulses were transformed into visibility graphs\\nas in Figure 1, and the corresponding adjacency matrices were\\nconverted into images to feed the VGG19 model which was\\npre-trained on the ImageNet dataset. Figure 12 illustrates the\\nthree-channel image input which is required for the VGG19\\nmodel; the three inputs were the adjacency matrix generated\\nfrom the PPG pulses, the adjacency matrix generated from the\\namplitude-inverted PPG pulses, and the adjacency matrix of a\\nslope-weighted visibility graph generated from the amplitude-\\ninverted PPG signal. For slope-weighted visibility graphs, the\\nedge weights (the non-zero values of the adjacency matrices)\\ndepend on the slope of the line connecting the two data points.\\nThese different modes of information were fused by consider-\\ning them as the RGB channels. With these channels as the RGB\\nchannels of a colour image, Figure 11 shows an example of a\\nresulting input image of the VGG19 model, with no preprocess-\\ning required. The findpeaks function in MATLAB was used to\\nextract pulses from the PPG signals. Then, the adjacency matri-\\nces were shuffled and divided into the training, validation, and\\ntest sets with sizes of 9,148, 3,050, and 3,049. Ten consecutive\\ntests were performed using different training/validation/test dis-\\ntributions (different random seeds). The results were analysed\\nby calculating the mean and standard deviation.\\nAs for the model architecture, the VGG19 model was pre-\\ntrained on the ImageNet dataset with 1,000 classes. Thus, a\\nfully connected top layer with the size of 1000√ó 4, which gives\\na four dimension output suitable for classifying subjects into\\nfour age groups, was added to the VGG19 model for the clas-\\nsification task. In addition, the top layers of size 1000 √ó 1 and\\n1000 √ó 2 were included for the regression and binary classifica-\\ntion task according to the required output dimension.\\nFigure 11: The RGB version of the input visibility graphs adjacency matrices.\\nThe RGB channels from Figure 12 are combined to form this image. Di fferent\\nmodes of information are fused and form an RGB image.\\nFirst Channel: Inverted PPG and Corresponding Adjacency Matrix\\n-150\\n-100\\n-50\\n0\\n50\\n100\\n150\\n200\\n0 10 20 30 40 50\\n(a)\\nSecond Channel: Original PPG and Corresponding Adjacency Matrix\\n-200\\n-150\\n-100\\n-50\\n0\\n50\\n100\\n150\\n0 10 20 30 40 50\\n(b)\\nThird Channel: Inverted PPG and Corresponding Slope-Weighted Adjacency M atrix\\n-150\\n-100\\n-50\\n0\\n50\\n100\\n150\\n200\\n0 10 20 30 40 50\\n(c)\\nFigure 12: Three channels of a PPG pulse and their corresponding adjacency\\nmatrices. (a): Red channel: the adjacency matrix generated from the PPG\\npulse. (b): Green channel: the adjacency matrix generated from the amplitude-\\ninverted PPG pulse. (c): Blue channel: the adjacency matrix of a slope-\\nweighted visibility graph generated from the amplitude-inverted PPG pulse.\\n3.2.4. Results and Discussions\\nWe compared the proposed methods for the classification\\ntask with that introduced by Dall‚ÄôOlio et al. (Dall‚ÄôOlio et al.,\\n2020), which performed a binary classification into the young\\nand old. To make a fair comparison, we adopted the method\\nin (Dall‚ÄôOlio et al., 2020) to our dataset for the same four-class\\nclassification. We also applied our method to a binary classi-\\nfication task, classifying subjects into classes of below 30 and\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='above 30 in age. For the four-class classification task, the work\\nin (Dall‚ÄôOlio et al., 2020) uses windows of 15 pulses which\\ndid not converge for our dataset. Instead, our data segmenta-\\ntion scheme of 50 signal samples per window, was employed\\nin place of the original 15 peaks window scheme in (Dall‚ÄôOlio\\net al., 2020). For the training strategy, the learning rate at the\\nbeginning was 10‚àí5 and decreased by a factor of 10 if the loss\\nof the validation set did not decrease after 5 consecutive epochs.\\nThe training stopped if the validation loss did not decrease for\\n10 consecutive epochs. Tables 4 and 5 show the results of\\nthe proposed VGTL-net method and the comparison method\\nfrom Dall‚ÄôOlio et al. (Dall‚ÄôOlio et al., 2020). Observe that the\\nVGTL-net outperformed the comparison method in both mul-\\nticlass and binary classification problems. For the multiclass\\nclassification problem, the VGTL-net converged much faster\\nthan the comparison method. Figure 13 and 14 show the confu-\\nsion matrices for the multiclass and binary classification prob-\\nlems of one of the train/validation/test splits.\\nFigure 13: The confusion matrix of the performance of the VGTL-net on the\\nfour-class task of classifying subjects into age groups of 0-20, 20-30, 30-40,\\nand 40+ years old\\nFigure 14: The confusion matrix of the performance of the VGTL-net on the\\nbinary task of classifying subjects into age groups of 0-30 and 30+ years old.\\nIn the regression task, the proposed VGTL-net model demon-\\nstrated high performance from the perspective of average mean\\nabsolute error and standard deviation, with the values of 1.41\\nyears and 0.14 years, respectively. Figure 15 shows the pre-\\ndicted age and ground truth age of subjects in the test set. To\\nthe best of our knowledge, this is the first study to perform re-\\ngression on subjects‚Äô age.\\nIn the preprocessing part, the method proposed by Dall‚ÄôOlio\\net al. (Dall‚ÄôOlio et al., 2020) firstly removed the trend of the\\nPPG signal using a centered moving average. The so processed\\nPPG signal was then demodulated using a Hilbert transform and\\nthe envelope was extracted. The final version of the PPG signal\\nwas obtained by dividing the demodulated signal by the enve-\\nlope. In contrast, our method did not require any preprocessing\\nand simply used the findpeaks function in MATLAB. In addi-\\ntion, both methods used deep learning to classify the age group.\\nThe comparison method adapted the ResNet model from the\\ncomputer vision area to work for one-dimensional signals. On\\nthe other hand, our method is naturally suitable for much more\\ndeveloped convolutional neural networks for two-dimensional\\nimages, as it easily converts a 1D signal into a 2D signal, which\\nenables us to utilise the power of transfer learning, thus re-\\nsulting in faster convergence and better performance. Overall,\\nour proposed VGTL-net showed advantages in both preprocess-\\ning and learning, as VGTL-net required much less preprocess-\\ning and did not require complex human-designed and dataset-\\noriented rules.\\nMethod Accuracy: Four Classes Epochs\\nVGTL-net 97.20 ¬± 0.2393% 26.66 ¬± 3\\nDall‚ÄôOlio, et al. 87.14% 280\\nTable 4: Multiclass classification performances of the proposed VGTL-net and\\nthe method in (Dall‚ÄôOlio et al., 2020).\\nMethod Accuracy: Binary Classes Epochs\\nVGTL-net 95.7 ¬± 0.1375% 22 ¬± 1.35\\nDall‚ÄôOlio, et al. 95.3% Not Known\\nTable 5: Binary classification performances of the proposed VGTL-net and the\\nmethod in (Dall‚ÄôOlio et al., 2020).\\nFigure 15: The results of age prediction by the VGTL-net, against the ground\\ntruth age of the subjects in the test set.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='4. Discussion\\nWe have proposed a novel model called the VGTL-net (Vis-\\nibility Graph and Transfer Learning) that combines graph the-\\nory, computer vision and deep learning to analyse photoplethys-\\nmography (PPG) signals. The method first converts the PPG\\nsignals to visibility graphs and represents them as adjacency\\nmatrices that can easily be visualised as images. This trans-\\nforms the one-dimensional PPG signals into their two-dimensional\\nrepresentations, which are suitable as inputs to computer vi-\\nsion networks. We have demonstrated that visibility graphs can\\ncapture the physiological characteristics of their adjacency ma-\\ntrices, which underpins the VGTL-net model. Moreover, the\\naffine transformation invariant property of the visibility graph\\nmakes it robust for PPG signals that are constantly a ffected by\\nbaseline wandering and motion artefacts. Given that the visi-\\nbility graph only preserves the structural information and dis-\\ncards the amplitude information, this is advantageous in situ-\\nations where the amplitude is influenced by many exogenous\\nfactors, such as age, which typically yields misleading results.\\nAfter extracting the adjacency matrices as images, we applied\\nthe pretrained models to these images to produce the final out-\\nput. This enables one-dimensional PPG signals to leverage the\\npower of computer vision algorithms that only work on two-\\ndimensional images.\\nWe have tested the VGTL-net model on the prediction of\\nthe subjects‚Äô age and blood pressure from pure PPG signals,\\nand it has shown its powerful generalisation abilities by per-\\nforming classification and regression on two different tasks and\\ntwo different datasets. The VGTL-net model has demonstrated\\nits advantage of requiring fewer preprocessing steps, and the\\nabsence of any requirement to apply algorithms such as princi-\\npal component analysis (PCA) to retrieve information, or em-\\npirical mode decomposition (EMD) to single out signal com-\\nponents. This makes the model robust and straightforward to\\napply since many of the preprocessing algorithms are sensitive\\nto parameters and require human calibration. Another advan-\\ntage of VGTL-net is the elimination of the need for manually\\nextracting features from PPG signals, as the PPG signals are fed\\nto the VGTL-net model as a whole. This is critically important\\nfor signals of low quality that might a ffect feature extraction\\nalgorithms.\\nIn summary, the VGTL-net algorithm provides a simple yet\\npowerful framework that has good generalisation ability and\\nhas the potential to become a universal framework that is suit-\\nable for multiple applications, especially those based on physi-\\nological data\\nReferences\\nBaker, S., Xiang, W., Atkinson, I., 2021. A hybrid neural network for con-\\ntinuous and non-invasive estimation of blood pressure from raw electrocar-\\ndiogram and photoplethysmogram waveforms. Computer Methods and Pro-\\ngrams in Biomedicine 207, 106191.\\nBiagetti, G., Crippa, P., Falaschetti, L., Orcioni, S., Turchetti, C., 2019. Re-\\nduced complexity algorithm for heart rate monitoring from ppg signals us-\\ning automatic activity intensity classifier. Biomedical Signal Processing and\\nControl 52, 293‚Äì301.\\nDai, Z., Liu, H., Le, Q.V ., Tan, M., 2021. Coatnet: Marrying convolution\\nand attention for all data sizes. Advances in neural information processing\\nsystems 34, 3965‚Äì3977.\\nDall‚ÄôOlio, L., Curti, N., Remondini, D., Safi Harb, Y ., Asselbergs, F.W., Castel-\\nlani, G., Uh, H.W., 2020. Prediction of vascular aging based on smartphone\\nacquired PPG signals. Scientific Reports 10, 1‚Äì10.\\nDavies, H.J., Bachtiger, P., Williams, I., Molyneaux, P.L., Peters, N.S., Mandic,\\nD.P., 2022a. Wearable in-ear PPG: Detailed respiratory variations enable\\nclassification of COPD. IEEE Transactions on Biomedical Engineering 69,\\n2390‚Äì2400.\\nDavies, H.J., Mandic, D.P., 2023. Rapid extraction of respiratory waveforms\\nfrom photoplethysmography: A deep corr-encoder approach. Biomedical\\nSignal Processing and Control 85, 104992.\\nDavies, H.J., Williams, I., Hammour, G., Yarici, M., Stacey, M.J., Seemun-\\ngal, B.M., Mandic, D.P., 2022b. In-ear spo2 for classification of cognitive\\nworkload. IEEE Transactions on Cognitive and Developmental Systems 15,\\n950‚Äì958. doi:10.1109/TCDS.2022.3196841.\\nDavies, H.J., Williams, I., Peters, N.S., Mandic, D.P., 2020. In-ear spo2: a\\ntool for wearable, unobtrusive monitoring of core blood oxygen saturation.\\nSensors 20, 4879.\\nDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. Imagenet:\\nA large-scale hierarchical image database. Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition , 248‚Äì255.\\nEl-Hajj, C., Kyriacou, P.A., 2021. Cuffless blood pressure estimation from ppg\\nsignals and its derivatives using deep learning models. Biomedical Signal\\nProcessing and Control 70, 102984.\\nEsmaelpoor, J., Moradi, M.H., Kadkhodamohammadi, A., 2020. A multi-\\nstage deep neural network model for blood pressure estimation using photo-\\nplethysmogram signals. Computers in Biology and Medicine 120, 103719.\\nEysenbach, G., et al., 2001. What is e-health? Journal of medical Internet\\nresearch 3, e833.\\nHammour, G., Mandic, D.P., 2023. An in-ear PPG-based blood glucose moni-\\ntor: A proof-of-concept study. Sensors 23, 3319.\\nHuang, B., Chen, W., Lin, C.L., Juang, C.F., Wang, J., 2022. Mlp-bp: A novel\\nframework for cuffless blood pressure measurement with ppg and ecg signals\\nbased on mlp-mixer neural networks. Biomedical Signal Processing and\\nControl 73, 103404.\\nKachuee, M., Kiani, M.M., Mohammadzade, H., Shabany, M., 2015. Cu ff-\\nless high-accuracy calibration-free blood pressure estimation using pulse\\ntransit time. 2015 IEEE International Symposium on Circuits and Sys-\\ntems (ISCAS) , 1006‚Äì1009URL:https://api.semanticscholar.org/\\nCorpusID:30566868.\\nKachuee Mohamad, Kiani Mohammad, M.H., Mahdi, S., 2015. Cu ff-Less\\nBlood Pressure Estimation. UCI Machine Learning Repository. DOI:\\nhttps://doi.org/10.24432/C5B602.\\nLacasa, L., Luque, B., Ballesteros, F., Luque, J., Nuno, J.C., 2008. From time\\nseries to complex networks: The visibility graph. Proceedings of the Na-\\ntional Academy of Sciences 105, 4972‚Äì4975.\\nLong, W., Wang, X., 2023. Bpnet: A multi-modal fusion neural network for\\nblood pressure estimation using ecg and ppg. Biomedical Signal Processing\\nand Control 86, 105287.\\nMiao, F., Wen, B., Hu, Z., Fortino, G., Wang, X.P., Liu, Z.D., Tang, M., Li, Y .,\\n2020. Continuous blood pressure measurement from one-channel electro-\\ncardiogram signal using deep-learning techniques. Artificial Intelligence in\\nMedicine 108, 101919.\\nMotin, M.A., Karmakar, C.K., Palaniswami, M., 2017. Ensemble empirical\\nmode decomposition with principal component analysis: A novel approach\\nfor extracting respiratory rate and heart rate from photoplethysmographic\\nsignal. IEEE Journal of Biomedical and Health Informatics 22, 766‚Äì774.\\nMotin, M.A., Karmakar, C.K., Palaniswami, M., 2019. Selection of empiri-\\ncal mode decomposition techniques for extracting breathing rate from PPG.\\nIEEE Signal Processing Letters 26, 592‚Äì596.\\nParreira, J.D., Chalumuri, Y .R., Mousavi, A.S., Modak, M., Zhou, Y ., Sanchez-\\nPerez, J.A., Gazi, A.H., Harrison, A.B., Inan, O.T., Hahn, J.O., 2023.\\nA proof-of-concept investigation of multi-modal physiological signal re-\\nsponses to acute mental stress. Biomedical Signal Processing and Control\\n85, 105001.\\nPimentel, M.A., Charlton, P.H., Clifton, D.A., 2015. Probabilistic estimation\\nof respiratory rate from wearable sensors, in: Mukhopadhyay, S.C. (Ed.),\\nWearable Electronics Sensors. Springer, pp. 241‚Äì262.\\nRong, M., Li, K., 2021. A multi-type features fusion neural network for blood\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='pressure prediction based on photoplethysmography. Biomedical Signal\\nProcessing and Control 68, 102772.\\nShin, H., Noh, G., Choi, B.M., 2022. Photoplethysmogram based vascular\\naging assessment using the deep convolutional neural network. Scientific\\nReports 12, 11377.\\nSiam, A., 2019. Real-World PPG dataset.\\nSimonyan, K., Zisserman, A., 2014. Very deep convolutional networks for\\nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 .\\nSlapniÀácar, G., Mlakar, N., Lu Àástrek, M., 2019. Blood pressure estimation from\\nphotoplethysmogram using a spectro-temporal deep neural network. Sen-\\nsors 19, 3420.\\nSultan, M.A., Saadeh, W., 2023. Continuous patient-independent estimation of\\nrespiratory rate and blood pressure using robust spectro-temporal features\\nderived from photoplethysmogram only. IEEE Open Journal of Engineering\\nin Medicine and Biology .\\nSuzuki, S., Oguri, K., 2009. Cu ffless blood pressure estimation by error-\\ncorrecting output coding method based on an aggregation of Adaboost with a\\nphotoplethysmograph sensor. Proceedings of the Annual International Con-\\nference of the IEEE Engineering in Medicine and Biology Society , 6765‚Äì\\n6768.\\nTan, C., Sun, F., Kong, T., Zhang, W., Yang, C., Liu, C., 2018. A survey on deep\\ntransfer learning. Proceedings of the International Conference on Artificial\\nNeural Networks 11141, 270‚Äì279.\\nWang, C., Yang, F., Yuan, X., Zhang, Y ., Chang, K., Li, Z., 2020. An end-to-\\nend neural network model for blood pressure estimation using ppg signal, in:\\nArtificial Intelligence in China: Proceedings of the International Conference\\non Artificial Intelligence in China, Springer. pp. 262‚Äì272.\\nWoo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I.S., Xie, S., 2023.\\nConvnext v2: Co-designing and scaling convnets with masked autoen-\\ncoders, in: Proceedings of the IEEE /CVF Conference on Computer Vision\\nand Pattern Recognition, pp. 16133‚Äì16142.\\nYousef, Q., Reaz, M., Ali, M.A.M., 2012. The analysis of PPG morphology:\\nInvestigating the effects of aging on arterial compliance. Measurement Sci-\\nence Review 12, 266.\\nZhang, C., Jovanov, E., Liao, H., Zhang, Y .T., Lo, B., Zhang, Y ., Guan, C.,\\n2023. Video based cocktail causal container for blood pressure classifica-\\ntion and blood glucose prediction. IEEE Journal of Biomedical and Health\\nInformatics 27, 1118‚Äì1128. doi:10.1109/JBHI.2022.3220967.\\n12')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2305.14062.pdf\")\n",
    "pdf_doc=loader.load()\n",
    "pdf_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs\\nand Transfer Learning\\nYuyang Miaoa,‚àó, Harry J. Daviesa, Danilo P. Mandica\\na Department of Electrical and Electronic Engineering, Imperial College London, Exhibition Rd, South Kensington, London, SW7 2BX, United Kingdom\\nAbstract\\nPhotoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable\\ndevices. The PPG signals provide insight into the body‚Äôs circulatory system and can be employed to extract various bio-features,\\nsuch as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations,\\nincluding heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we\\nintroduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-01-17T04:11:30+00:00', 'author': 'Yuyang Miao; Harry J. Davies; Danilo P. Mandic;', 'keywords': '', 'moddate': '2024-01-17T04:11:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.14062.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis\\nframework which is amplitude-independent and invariant to a ffine transformations. It also requires minimal preprocessing, fuses\\ninformation through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves\\nstate-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation of continuous blood pressure\\nwaveforms.\\nKeywords: Photoplethysmography (PPG), Visibility Graph, Transfer Learning, Graph Theory\\n1. Introduction\\nWith the rapid development of technology, the integration\\nof technology into healthcare, denoted as E-health, is reshap-\\ning the ways in which the healthcare services are provided and\\naccessed (Eysenbach et al., 2001). E-health promises enhanced\\nefficiency in healthcare delivery, potentially reducing the cost\\nwhile simultaneously improving the quality of care.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "texts = text_splitter.split_documents(pdf_doc)\n",
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_630659/2955505664.py:6: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  OllamaEmbeddings(model=\"llama3.2:1b\"),\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    texts[:6],\n",
    "    OllamaEmbeddings(model=\"llama3.2:1b\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VGTL-net: Amplitude-Independent Machine Learning for PPG through Visibility Graphs\\nand Transfer Learning\\nYuyang Miaoa,‚àó, Harry J. Daviesa, Danilo P. Mandica\\na Department of Electrical and Electronic Engineering, Imperial College London, Exhibition Rd, South Kensington, London, SW7 2BX, United Kingdom\\nAbstract\\nPhotoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable\\ndevices. The PPG signals provide insight into the body‚Äôs circulatory system and can be employed to extract various bio-features,\\nsuch as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations,\\nincluding heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we\\nintroduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"An additional easily observable feature of PPG is the heart rate (HR)what is Signal To Graph: Visibility Graph\"\n",
    "res=db.similarity_search(query)\n",
    "res[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based on the context provided. If the answer is not in the context, say \"I don't know\n",
    "    think step by step before answering provide detail answer\".\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, I don\\'t know. \\n\\nThe author of the paper mentions that they introduce a PPG signal processing framework called \"Signal To Graph: Visibility Graph\" in their introduction. However, they do not explicitly define or explain what \"Visibility Graph\" is within this specific framework.\\n\\nTo provide a detailed answer, I would need to look beyond the provided context and research papers. \\n\\nFrom my understanding of graph theory and computer vision algorithms, a Visibility Graph (VG) refers to a data structure used in various applications, including image processing, object recognition, and machine learning. It is a way of representing complex data as a graph where vertices represent data points or objects, and edges connect related or adjacent elements.\\n\\nIn the context of signal processing, particularly PPG signals, a Visibility Graph could potentially be used to analyze and process the signal in a more efficient and effective manner. However, without further information from the authors, I cannot provide a definitive answer on what exactly \"Visibility Graph\" represents in this specific framework.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Define the prompt template with 'context' and 'input' variables\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based on the context provided. If the answer is not in the context, say \"I don't know.\"\n",
    "    Think step by step before answering and provide a detailed answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize the Ollama language model\n",
    "llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "# Create the document chain, specifying 'context' as the document variable name\n",
    "doc_chain = create_stuff_documents_chain(\n",
    "    llm,\n",
    "    prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever\n",
    "\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,doc_chain)\n",
    "\n",
    "res=retrieval_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"what is Signal To Graph: Visibility Graph\"\n",
    "    }\n",
    ")\n",
    "res[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
