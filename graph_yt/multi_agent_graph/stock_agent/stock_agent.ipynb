{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "753ebcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import create_react_agent\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate,PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "from langchain.tools import  tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d678fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]= GROQ_API_KEY\n",
    "\n",
    "# llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "llm=ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9cc6640a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 11, 'total_tokens': 36, 'completion_time': 0.071428571, 'prompt_time': 0.000127359, 'queue_time': 0.0547244, 'total_time': 0.07155593}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run-079605c2-4ecc-401c-ab8a-8b6d77c3a40c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 25, 'total_tokens': 36})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "92f236d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_tool(query: str) -> str:\n",
    "    \"\"\"A simple search tool that returns a fixed response.\"\"\"\n",
    "    tavily_search = TavilySearchResults()\n",
    "    results = tavily_search.invoke(query)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3f40f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'List of capitals of France - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/List_of_capitals_of_France', 'content': 'Find sources:\\xa0\"List of capitals of France\"\\xa0–\\xa0news\\xa0· newspapers\\xa0· books\\xa0· scholar\\xa0· JSTOR (July 2012) (Learn how and when to remove this message)\\nThis is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944.[1]\\nChronology[edit]\\nTournai (before 486), current-day Belgium\\nSoissons (486–936)\\nLaon (936–987)\\nParis (987–1419), the residence of the Kings of France, although they were consecrated at Reims. [...] Bordeaux (September 1914), the government was relocated from Paris to Bordeaux very briefly during World War I, when it was feared that Paris would soon fall into German hands. These fears were alleviated after the German Army was pushed back at the First Battle of the Marne.\\nTours (10–13 June 1940), the city served as the temporary capital of France during World War II after the government fled Paris due to the German advance. [...] Paris (1789–1871), on 5 and 6 October 1789, a throng from Paris invaded the castle and forced the royal family to move back to Paris. The National Constituent Assembly followed the King to Paris soon afterward; Versailles lost its role of capital city.\\nProvisional seats of the government:\\nVersailles (1871), the French Third Republic established Versailles as its provisional seat of government in March 1871 after the Paris Commune took control of Paris.', 'score': 0.92604345}, {'title': 'Paris facts: the capital of France in history', 'url': 'https://home.adelphi.edu/~ca19535/page%204.html', 'content': 'Home    Spain\\n    Sydney\\n    San Francisco\\n    Paris\\n    Las Vegas\\n    Maui\\nParis, France\\nParis facts: Paris, the capital of France\\nParis is the capital of France, the largest country of Europe with 550 000 km2 (65 millions inhabitants).\\nParis has 2.234 million inhabitants end 2011. She is the core of Ile de France region (12 million people). [...] Paris facts: the capital of France in history\\nBefore Paris, the capital of France was Lyon (under the Romans). Paris first became the capital of France in 508 under King Clovis. After centuries with no unique capital of France, Paris retrieved its status of capital of France under King Philippe Auguste, who reigned between 1180 and 1223. You can see remains of the Philippe August Paris walls in the passageway between the Louvre parking and Louvre Museum [...] Paris remained the capital of France until today, with one four year interruption. During German occupation (WW2 , 1940-1944), the capital of France was Vichy.\\ngo to top\\nReference: http://www.parisdigest.com/information/facts.htm', 'score': 0.89720124}, {'title': 'What is the capital of France? Overview of Paris - iRoamly Travel eSIM', 'url': 'https://www.iroamly.com/france-travel/what-is-the-capital-of-france.html', 'content': \"Written by\\xa0Isabella Torres\\nNov 28, 2024 • 4-min read\\nListen\\nListen\\nThe capital of France?\\nWelcome to Paris – more than just a city, it’s an odyssey in itself. The “City of Light,” the “City of Love,” and much, much more.\\nLet's get to the heart of a place that continues to capture the minds and spirits of people around the world.\\nBefore you arrive, make sure to equip yourself with an iRoamly France travel eSIM to stay connected and share your adventures in real-time. [...] Published Time: 2024-11-11T18:55:59-00:00\\nWhat is the capital of France? Overview of Paris\\n\\n\\nDestination\\nSupport\\nTravel Tips\\n\\nCurrency Converter\\n\\n\\nSign Up\\n\\n\\nUsername\\n\\nMy eSIM\\niMoney $ 0.00\\nRefer & Earn\\nAccount Info\\n\\nLog out\\n\\n\\niMoney\\n\\nTransaction\\nAccount Info\\nPayout\\nMy eSIM\\nLog out\\n\\n\\n\\nEnglish [...] What Is the Capital of France?\\nParis. The very name brings to mind tree-lined boulevards, grand monuments, sidewalk cafes, and so much more. Spread over an area of around 105 square kilometers and standing around 130 meters above sea level, Paris is simultaneously approachable and awe-inspiring.\\nNicknames of Paris\", 'score': 0.8740022}, {'title': 'Paris | Definition, Map, Population, Facts, & History - Britannica', 'url': 'https://www.britannica.com/place/Paris', 'content': \"Paris is the capital of what country?\\nParis is the national capital of France.\\nNews •\\nJD Vance will attend AI summit in Paris and Munich security conference in first overseas trip as VP • Feb. 4, 2025, 11:53 AM ET (AP) ...(Show more)\\nFilm director found guilty of sexual assault in France’s first big #MeToo trial • Feb. 3, 2025, 9:20 AM ET (AP)\\nGisèle Pelicot's ex-husband, imprisoned for raping and drugging her, now caught up in other cases • Jan. 30, 2025, 11:56 AM ET (AP)\", 'score': 0.8724455}, {'title': 'France | History, Maps, Flag, Population, Cities, Capital, & Facts', 'url': 'https://www.britannica.com/place/France', 'content': 'The capital and by far the most important city of France is Paris, one of the world’s preeminent cultural and commercial centres. A majestic city known as the ville lumière, or “city of light,” Paris has often been remade, most famously in the mid-19th century under the command of Georges-Eugène, Baron Haussman, who was committed to Napoleon III’s vision of a modern city free of the choleric swamps and congested alleys of old, with broad avenues and a regular plan. Paris is now a sprawling [...] See article: flag of France\\nAudio File: Anthem of France (see article)\\nHead Of Government:\\nPrime minister: Michel Barnier\\n(Show\\xa0more)\\nCapital:\\nParis\\n(Show\\xa0more)\\nPopulation:\\n(2025 est.) 66,607,000\\n(Show\\xa0more)\\nCurrency Exchange Rate:\\n1 USD equals 0.937 euro\\n(Show\\xa0more)\\nHead Of State:\\nPresident: Emmanuel Macron\\n(Show\\xa0more)\\nForm Of Government:\\nrepublic with two legislative houses (Parliament; Senate [348], National Assembly [577])\\n(Show\\xa0more)\\nOfficial Language:\\nFrench\\n(Show\\xa0more)', 'score': 0.8571119}]\n"
     ]
    }
   ],
   "source": [
    "print(search_tool(\"what is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d449cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance\n",
    "@tool\n",
    "def get_stock_price(ticker:str):\n",
    "    \"\"\"A simple stock price tool that returns a fixed response.\"\"\"\n",
    "    stock = yfinance.Ticker(ticker)\n",
    "    stock_price = stock.history()\n",
    "    return stock_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27b8e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Open        High         Low       Close  \\\n",
      "Date                                                                        \n",
      "2025-03-12 00:00:00-04:00  220.139999  221.750000  214.910004  216.979996   \n",
      "2025-03-13 00:00:00-04:00  215.949997  216.839996  208.419998  209.679993   \n",
      "2025-03-14 00:00:00-04:00  211.250000  213.949997  209.580002  213.490005   \n",
      "2025-03-17 00:00:00-04:00  213.309998  215.220001  209.970001  214.000000   \n",
      "2025-03-18 00:00:00-04:00  214.160004  215.149994  211.490005  212.690002   \n",
      "2025-03-19 00:00:00-04:00  214.220001  218.759995  213.750000  215.240005   \n",
      "2025-03-20 00:00:00-04:00  213.990005  217.490005  212.220001  214.100006   \n",
      "2025-03-21 00:00:00-04:00  211.559998  218.839996  211.279999  218.270004   \n",
      "2025-03-24 00:00:00-04:00  221.000000  221.479996  218.580002  220.729996   \n",
      "2025-03-25 00:00:00-04:00  220.770004  224.100006  220.080002  223.750000   \n",
      "2025-03-26 00:00:00-04:00  223.509995  225.020004  220.470001  221.529999   \n",
      "2025-03-27 00:00:00-04:00  221.389999  224.990005  220.559998  223.850006   \n",
      "2025-03-28 00:00:00-04:00  221.669998  223.809998  217.679993  217.899994   \n",
      "2025-03-31 00:00:00-04:00  217.009995  225.619995  216.229996  222.130005   \n",
      "2025-04-01 00:00:00-04:00  219.809998  223.679993  218.899994  223.190002   \n",
      "2025-04-02 00:00:00-04:00  221.320007  225.190002  221.020004  223.889999   \n",
      "2025-04-03 00:00:00-04:00  205.539993  207.490005  201.250000  203.190002   \n",
      "2025-04-04 00:00:00-04:00  193.889999  199.880005  187.339996  188.380005   \n",
      "2025-04-07 00:00:00-04:00  177.199997  194.149994  174.619995  181.460007   \n",
      "2025-04-08 00:00:00-04:00  186.699997  190.339996  169.210007  172.419998   \n",
      "2025-04-09 00:00:00-04:00  171.949997  200.610001  171.889999  198.850006   \n",
      "2025-04-10 00:00:00-04:00  189.070007  194.779999  183.000000  190.419998   \n",
      "2025-04-11 00:00:00-04:00  186.100006  199.539993  186.059998  198.149994   \n",
      "\n",
      "                              Volume  Dividends  Stock Splits  \n",
      "Date                                                           \n",
      "2025-03-12 00:00:00-04:00   62547500        0.0           0.0  \n",
      "2025-03-13 00:00:00-04:00   61368300        0.0           0.0  \n",
      "2025-03-14 00:00:00-04:00   60107600        0.0           0.0  \n",
      "2025-03-17 00:00:00-04:00   48073400        0.0           0.0  \n",
      "2025-03-18 00:00:00-04:00   42432400        0.0           0.0  \n",
      "2025-03-19 00:00:00-04:00   54385400        0.0           0.0  \n",
      "2025-03-20 00:00:00-04:00   48862900        0.0           0.0  \n",
      "2025-03-21 00:00:00-04:00   94127800        0.0           0.0  \n",
      "2025-03-24 00:00:00-04:00   44299500        0.0           0.0  \n",
      "2025-03-25 00:00:00-04:00   34493600        0.0           0.0  \n",
      "2025-03-26 00:00:00-04:00   34466100        0.0           0.0  \n",
      "2025-03-27 00:00:00-04:00   37094800        0.0           0.0  \n",
      "2025-03-28 00:00:00-04:00   39818600        0.0           0.0  \n",
      "2025-03-31 00:00:00-04:00   65299300        0.0           0.0  \n",
      "2025-04-01 00:00:00-04:00   36412700        0.0           0.0  \n",
      "2025-04-02 00:00:00-04:00   35905900        0.0           0.0  \n",
      "2025-04-03 00:00:00-04:00  103419000        0.0           0.0  \n",
      "2025-04-04 00:00:00-04:00  125910900        0.0           0.0  \n",
      "2025-04-07 00:00:00-04:00  160466300        0.0           0.0  \n",
      "2025-04-08 00:00:00-04:00  120859500        0.0           0.0  \n",
      "2025-04-09 00:00:00-04:00  184395900        0.0           0.0  \n",
      "2025-04-10 00:00:00-04:00  121880000        0.0           0.0  \n",
      "2025-04-11 00:00:00-04:00   87300000        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "print(get_stock_price(\"AAPL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fd8ad79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def summarizer(state:MessagesState):\n",
    "    \"\"\"A simple summarizer that returns a good and user friendly response.\"\"\"\n",
    "    messages=state['messages']\n",
    "    print(\"---msg---\",messages)\n",
    "    last_message=messages[-1]\n",
    "    print(\"---last---\",last_message)\n",
    "    prompt=ChatPromptTemplate.from_template(\n",
    "            \"\"\"You are an intelligent, helpful, and professional assistant. Your task is to respond to the user's query in a thoughtful, clear, and well-structured manner. Make sure the final response is:\n",
    "\n",
    "            Accurate and directly addresses the user's question or request.\n",
    "\n",
    "            Written in a friendly yet professional tone.\n",
    "\n",
    "            Easy to understand, avoiding jargon unless necessary (and explain any complex terms).\n",
    "\n",
    "            Well-organized, with logical flow and, if needed, use bullet points, headings, or numbered steps for clarity.\n",
    "\n",
    "            Always think deeply about the user’s needs, context, and intent. Aim to provide a high-quality final answer that the user can trust and rely on.\n",
    "            Query:{question}\n",
    "            \"\"\")\n",
    "    chain = prompt | llm\n",
    "#     response=chain.invoke({\"question\":last_message})\n",
    "    response=chain.invoke({\"question\":messages})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bdb9d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---msg--- what is the capital of France?\n",
      "---last--- ?\n"
     ]
    }
   ],
   "source": [
    "res=summarizer({\"messages\": \"what is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "492fdf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour!\\n\\nThe capital of France is Paris (French pronunciation: \\u200b[paʁi]). Located in the north-central part of the country, Paris is not only the capital but also the most populous city in France, with a rich history dating back to the 3rd century. Known as the \"City of Light\" (La Ville Lumière), Paris is famous for its stunning architecture, art museums, fashion, and romantic atmosphere.\\n\\nSome interesting facts about Paris:\\n\\n• The Eiffel Tower, built for the 1889 World\\'s Fair, is one of the most iconic landmarks in Paris and a symbol of French culture.\\n• The Louvre Museum, home to the Mona Lisa, is one of the world\\'s largest and most famous museums.\\n• Paris is often called the culinary capital of the world, with its exquisite cuisine, wine, and bakeries.\\n\\nI hope this answers your question! Do you have any other queries about France or Paris?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 157, 'total_tokens': 350, 'completion_time': 0.551428571, 'prompt_time': 0.004881095, 'queue_time': 0.05341966, 'total_time': 0.556309666}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run-a5ce4dda-85b9-4728-98ed-7643473811fa-0', usage_metadata={'input_tokens': 157, 'output_tokens': 193, 'total_tokens': 350})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f5814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ff593c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def retriever_call(state: dict):\n",
    "    \"\"\"\n",
    "    A retriever function that accepts multiple URLs, scrapes them using WebBaseLoader,\n",
    "    and saves the cleaned data into a vector database.\n",
    "    \"\"\"\n",
    "    messages = state['messages']\n",
    "\n",
    "    # Extract last message (assuming it includes multiple URLs)\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # Optional: handle string with multiple URLs separated by comma, newline, or space\n",
    "    if isinstance(last_message, str):\n",
    "        urls = [url.strip() for url in last_message.replace(',', '\\n').splitlines() if url.strip()]\n",
    "    else:\n",
    "        urls = last_message  # if already a list\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are a smart assistant integrated with LangChain tools. The user has provided multiple URLs. \n",
    "For each URL, you need to:\n",
    "\n",
    "1. Use `WebBaseLoader` to load the content from the web page.\n",
    "2. Clean and extract meaningful text from the loaded content (e.g., remove navigation, ads, etc.).\n",
    "3. Save all the cleaned documents into a vector store using a suitable embedding model (like `OpenAIEmbeddings` or `HuggingFaceEmbeddings`).\n",
    "4. Ensure that the documents are indexed correctly for retrieval later.\n",
    "\n",
    "Here are the URLs to process:\n",
    "{urls}\n",
    "\n",
    "Once done, confirm that the content from all URLs has been scraped and saved successfully to the vector database.\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"urls\": \"\\n\".join(urls)})\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "db061e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def vectordb_generate(urls: list):\n",
    "    \"\"\"\n",
    "    A retriever function that accepts multiple URLs, scrapes them using WebBaseLoader,\n",
    "    splits the content, and saves it into a vector database.\n",
    "    \"\"\"\n",
    "    print(\"---urls---\", urls)\n",
    "    print(\"------VectorDB------\")\n",
    "\n",
    "    # Load and flatten documents\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        loaded_docs = WebBaseLoader(url).load()\n",
    "        docs.extend(loaded_docs)  # Flatten the nested list\n",
    "\n",
    "    print(f\"---Loaded {len(docs)} documents---\")\n",
    "\n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Save to Chroma vector store\n",
    "    vectordb = Chroma.from_documents(texts, embeddings, persist_directory=\"stock_test_chroma\")\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bc4b6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---urls--- ['https://www.bbc.com/news/world-us-canada-67012345', 'http://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration']\n",
      "------VectorDB------\n",
      "---Loaded 2 documents---\n"
     ]
    }
   ],
   "source": [
    "vector=vectordb_generate([\"https://www.bbc.com/news/world-us-canada-67012345\",\"http://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd2e6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "162c00eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'language': 'en-GB', 'source': 'https://www.bbc.com/news/world-us-canada-67012345', 'title': 'BBC'}, page_content='BBCSkip to contentBritish Broadcasting CorporationHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveHomeNewsIsrael-Gaza WarWar in UkraineUS & CanadaUKUK PoliticsEnglandN. IrelandN. Ireland PoliticsScotlandScotland PoliticsWalesWales PoliticsAfricaAsiaChinaIndiaAustraliaEuropeLatin AmericaMiddle EastIn PicturesBBC InDepthBBC VerifySportBusinessExecutive LoungeTechnology of BusinessFuture of BusinessInnovationTechnologyScience & HealthArtificial IntelligenceAI v the MindCultureFilm & TVMusicArt & DesignStyleBooksEntertainment NewsArtsArts in MotionTravelDestinationsAfricaAntarcticaAsiaAustralia and PacificCaribbean & BermudaCentral AmericaEuropeMiddle EastNorth AmericaSouth AmericaWorld’s TableCulture & ExperiencesAdventuresThe SpeciaListEarthNatural WondersWeather & ScienceClimate SolutionsSustainable BusinessGreen LivingAudioPodcastsRadioAudio FAQsVideoLiveLive NewsLive SportHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveWeatherNewslettersError 404'),\n",
       " Document(metadata={'language': 'en-GB', 'source': 'https://www.bbc.com/news/world-us-canada-67012345', 'title': 'BBC'}, page_content='BBCSkip to contentBritish Broadcasting CorporationHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveHomeNewsIsrael-Gaza WarWar in UkraineUS & CanadaUKUK PoliticsEnglandN. IrelandN. Ireland PoliticsScotlandScotland PoliticsWalesWales PoliticsAfricaAsiaChinaIndiaAustraliaEuropeLatin AmericaMiddle EastIn PicturesBBC InDepthBBC VerifySportBusinessExecutive LoungeTechnology of BusinessFuture of BusinessInnovationTechnologyScience & HealthArtificial IntelligenceAI v the MindCultureFilm & TVMusicArt & DesignStyleBooksEntertainment NewsArtsArts in MotionTravelDestinationsAfricaAntarcticaAsiaAustralia and PacificCaribbean & BermudaCentral AmericaEuropeMiddle EastNorth AmericaSouth AmericaWorld’s TableCulture & ExperiencesAdventuresThe SpeciaListEarthNatural WondersWeather & ScienceClimate SolutionsSustainable BusinessGreen LivingAudioPodcastsRadioAudio FAQsVideoLiveLive NewsLive SportHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveWeatherNewslettersError 404'),\n",
       " Document(metadata={'language': 'en-GB', 'source': 'https://www.bbc.com/news/world-us-canada-67012345', 'title': 'BBC'}, page_content='BBCSkip to contentBritish Broadcasting CorporationHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveHomeNewsIsrael-Gaza WarWar in UkraineUS & CanadaUKUK PoliticsEnglandN. IrelandN. Ireland PoliticsScotlandScotland PoliticsWalesWales PoliticsAfricaAsiaChinaIndiaAustraliaEuropeLatin AmericaMiddle EastIn PicturesBBC InDepthBBC VerifySportBusinessExecutive LoungeTechnology of BusinessFuture of BusinessInnovationTechnologyScience & HealthArtificial IntelligenceAI v the MindCultureFilm & TVMusicArt & DesignStyleBooksEntertainment NewsArtsArts in MotionTravelDestinationsAfricaAntarcticaAsiaAustralia and PacificCaribbean & BermudaCentral AmericaEuropeMiddle EastNorth AmericaSouth AmericaWorld’s TableCulture & ExperiencesAdventuresThe SpeciaListEarthNatural WondersWeather & ScienceClimate SolutionsSustainable BusinessGreen LivingAudioPodcastsRadioAudio FAQsVideoLiveLive NewsLive SportHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveWeatherNewslettersError 404'),\n",
       " Document(metadata={'description': 'Build language agents as graphs', 'language': 'en', 'source': 'http://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration', 'title': 'Multi-agent network'}, page_content='growth of Scotland 2021, by local area\\\\\\\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\\\\\\\nGDP growth of Wales 2021, by local area\\\\\\\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\\\\\\\nGDP growth of Northern Ireland 2021, by local area\\\\\\\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\\\\\\\nGDP per capita\\\\\\\\nGDP per capita\\\\\\\\nGDP per capita in the UK 1955-2022\\\\\\\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\\\\\\\nAnnual GDP per capita growth in the UK 1956-2022\\\\\\\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\\\\\\\nQuarterly GDP per capita in the UK 2019-2023\\\\\\\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\\\\\nQuarterly GDP per capita growth in the UK 2019-2023\\\\\\\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Any new about the canada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d79b330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool=create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retriever_tool\",\n",
    "    description=\"A tool to get Data about the query from the vetor database use it for any query related to the data in the vector database\",\n",
    "    # return_only_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6aaec3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "def safe_message_trimmer(messages: list, max_len: int = 4000) -> list:\n",
    "    \"\"\"Trims list of messages to fit within token limits (approx by character length).\"\"\"\n",
    "    def get_text(msg):\n",
    "        return msg.content if isinstance(msg, BaseMessage) else str(msg)\n",
    "\n",
    "    total_tokens = sum(len(get_text(m)) for m in messages)\n",
    "    while total_tokens > max_len and len(messages) > 1:\n",
    "        messages.pop(0)\n",
    "        total_tokens = sum(len(get_text(m)) for m in messages)\n",
    "    return messages\n",
    "\n",
    "trim_tool = Tool(\n",
    "    name=\"safe_message_trimmer\",\n",
    "    func=lambda msgs: safe_message_trimmer(msgs),\n",
    "    description=\"Trims chat history to avoid exceeding token limits. Use this when messages are too long.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c4582c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "# ReAct Agent Prompt\n",
    "\n",
    "You are an intelligent AI assistant with access to various tools to help answer user questions accurately. Your goal is to provide helpful, accurate, and concise responses.\n",
    "\n",
    "## Tools Available\n",
    "{tools}\n",
    "{tool_names}\n",
    "You have the following tools at your disposal:\n",
    "\n",
    "1. `retriever_call`: Retrieves information from a vector database of documents\n",
    "   - Input: A query string related to the documents in the knowledge base\n",
    "   - Output: Relevant document chunks from the knowledge base\n",
    "\n",
    "2. `search_tool`: Searches the web for recent or specific information\n",
    "   - Input: A search query\n",
    "   - Output: Search results including snippets and URLs\n",
    "\n",
    "3. `get_stock_price`: Fetches current or historical stock price information\n",
    "   - Input: Stock ticker symbol and optional date\n",
    "   - Output: Stock price data\n",
    "\n",
    "4. `summarizer`: Summarizes long texts or documents\n",
    "   - Input: Text content to summarize\n",
    "   - Output: Concise summary of the input text\n",
    "\n",
    "5. `retriever_tool`: Alternative retriever to access different document collections\n",
    "   - Input: A specific query for specialized document retrieval\n",
    "   - Output: Relevant document chunks from specialized collections\n",
    "\n",
    "6. `safe_message_trimmer`: Trims chat history to stay within token limits  \n",
    "   - Input: A list of messages  \n",
    "   - Output: A shortened list that fits within the model’s max context length\n",
    "                                            \n",
    "\n",
    "## How to Use Tools\n",
    "\n",
    "For each user question, follow this process:\n",
    "\n",
    "1. **Thought**: First, analyze what the user is asking and determine which tool(s) might be helpful.\n",
    "2. **Action**: Select a tool to use and provide the appropriate input.\n",
    "3. **Observation**: Review the information returned by the tool.\n",
    "4. **Thought**: Reflect on whether the information answers the question or if additional tools/queries are needed.\n",
    "5. Repeat steps 2-4 as necessary until you have enough information.\n",
    "6. **Answer**: Provide a final answer that directly addresses the user's question, citing sources when appropriate.\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "- Think step-by-step - don't jump to conclusions or use tools without reasoning first\n",
    "- Use retrieval tools before generating information from memory when factual accuracy is important\n",
    "- When using search or retrieval tools, craft specific queries that target the exact information needed\n",
    "- Only use tools when necessary - if you can confidently answer without tools, do so\n",
    "- Always verify information from tools before presenting it as fact\n",
    "- Keep your final answer concise but complete\n",
    "- When using information from retrieval or search tools, cite the source\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='retriever_call', description='A retriever function that accepts multiple URLs, scrapes them using WebBaseLoader,\\nand saves the cleaned data into a vector database.', args_schema=<class 'langchain_core.utils.pydantic.retriever_call'>, func=<function retriever_call at 0x7973037a2050>),\n",
       " StructuredTool(name='search_tool', description='A simple search tool that returns a fixed response.', args_schema=<class 'langchain_core.utils.pydantic.search_tool'>, func=<function search_tool at 0x7973037a3130>),\n",
       " StructuredTool(name='get_stock_price', description='A simple stock price tool that returns a fixed response.', args_schema=<class 'langchain_core.utils.pydantic.get_stock_price'>, func=<function get_stock_price at 0x7973037a2560>),\n",
       " StructuredTool(name='summarizer', description='A simple summarizer that returns a good and user friendly response.', args_schema=<class 'langchain_core.utils.pydantic.summarizer'>, func=<function summarizer at 0x7973037a0e50>),\n",
       " Tool(name='retriever_tool', description='A tool to get Data about the query from the vetor database use it for any query related to the data in the vector database', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x797339527ac0>, retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x797303930910>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x797339527c70>, retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x797303930910>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')),\n",
       " Tool(name='safe_message_trimmer', description='Trims chat history to avoid exceeding token limits. Use this when messages are too long.', func=<function <lambda> at 0x7971fa25df30>)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [\n",
    "    retriever_call,\n",
    "    search_tool,\n",
    "    get_stock_price,\n",
    "    summarizer,\n",
    "    retriever_tool,\n",
    "    trim_tool  # ✅ Our new safe input tool\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9d8cdd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    llm_with_tools,\n",
    "    tools=tools,\n",
    "    prompt=REACT_PROMPT,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False  # ✅ This matters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "231a9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_trim_messages(messages, max_chars=3500):\n",
    "    trimmed = []\n",
    "    total = 0\n",
    "    for msg in reversed(messages):\n",
    "        content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "        if total + len(content) <= max_chars:\n",
    "            trimmed.insert(0, msg)\n",
    "            total += len(content)\n",
    "    return trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "860e8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Please reduce the length of the messages or completion.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you get me the latest news about the Canada?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     21\u001b[0m messages \u001b[38;5;241m=\u001b[39m super_trim_messages(messages, max_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3500\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Just send the latest trimmed input\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain/agents/agent.py:1624\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1624\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1633\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1634\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain/agents/agent.py:1330\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1330\u001b[0m         [\n\u001b[1;32m   1331\u001b[0m             a\n\u001b[1;32m   1332\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1333\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1334\u001b[0m                 color_mapping,\n\u001b[1;32m   1335\u001b[0m                 inputs,\n\u001b[1;32m   1336\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1337\u001b[0m                 run_manager,\n\u001b[1;32m   1338\u001b[0m             )\n\u001b[1;32m   1339\u001b[0m         ]\n\u001b[1;32m   1340\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain/agents/agent.py:1330\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1330\u001b[0m         [\n\u001b[1;32m   1331\u001b[0m             a\n\u001b[1;32m   1332\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1333\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1334\u001b[0m                 color_mapping,\n\u001b[1;32m   1335\u001b[0m                 inputs,\n\u001b[1;32m   1336\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1337\u001b[0m                 run_manager,\n\u001b[1;32m   1338\u001b[0m             )\n\u001b[1;32m   1339\u001b[0m         ]\n\u001b[1;32m   1340\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain/agents/agent.py:1358\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1358\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain/agents/agent.py:465\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:3409\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   3404\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3405\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   3406\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3407\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3409\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:3396\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   3391\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3392\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   3393\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3396\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   3397\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   3399\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   3400\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3401\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2199\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 2199\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:3359\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3357\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[0;32m-> 3359\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1413\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1410\u001b[0m final: Input\n\u001b[1;32m   1411\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[1;32m   1422\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:5565\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   5560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5561\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   5562\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5563\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   5564\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 5565\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   5566\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5568\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5569\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1431\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:428\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/langchain_groq/chat_models.py:534\u001b[0m, in \u001b[0;36mChatGroq._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m    533\u001b[0m default_chunk_class: Type[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[0;32m--> 534\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    536\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/groq/resources/chat/completions.py:322\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/groq/_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1213\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1222\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1223\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1224\u001b[0m     )\n\u001b[0;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/groq/_base_client.py:917\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Learning/AI/graph_yt/env/lib/python3.10/site-packages/groq/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1029\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Please reduce the length of the messages or completion.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# messages= [\n",
    "#             HumanMessage(\n",
    "#                 content=\"Can you get me the latest news about the Canada?\"\n",
    "#             ),\n",
    "#         ],\n",
    "# messages = safe_message_trimmer(messages)\n",
    "\n",
    "\n",
    "\n",
    "# agent_executor.invoke(\n",
    "#     {\n",
    "#         \"input\": \"Can you get me the latest news about the Canada?\",\n",
    "#         \"messages\": messages,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Can you get me the latest news about the Canada?\")\n",
    "]\n",
    "\n",
    "messages = super_trim_messages(messages, max_chars=3500)\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": messages[-1].content  # Just send the latest trimmed input\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
